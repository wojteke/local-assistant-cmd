2023-06-17 14:46:36.069185: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
Found cached dataset json (/root/.cache/huggingface/datasets/json/default-1780e084e2d96dd4/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4)
100% 1/1 [00:00<00:00, 676.06it/s]
Loading cached split indices for dataset at /root/.cache/huggingface/datasets/json/default-1780e084e2d96dd4/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4/cache-f8ed66d7c728ddf1.arrow and /root/.cache/huggingface/datasets/json/default-1780e084e2d96dd4/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4/cache-145e6d94b2cd00b8.arrow
Token indices sequence length is longer than the specified maximum sequence length for this model (1050 > 1024). Running this sequence through the model will result in indexing errors
100% 100/100 [09:11<00:00,  5.52s/it]
/content/ai-assistant/evaluation.py:67: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ðŸ¤— Evaluate: https://huggingface.co/docs/evaluate
  sacrebleu = load_metric('sacrebleu')
Model: gpt2_epoch_2 BLEU Score: 93.34389232699647
100% 100/100 [10:27<00:00,  6.28s/it]
Model: gpt2_lora_epoch_2 BLEU Score: 87.75249537836979
100% 100/100 [16:56<00:00, 10.17s/it]
Model: gpt2-medium_epoch_2 BLEU Score: 93.50877701975215
100% 100/100 [20:18<00:00, 12.18s/it]
Model: gpt2-medium_lora_epoch_2 BLEU Score: 89.16482664853726
100% 100/100 [25:58<00:00, 15.58s/it]
Model: gpt2-large_epoch_2 BLEU Score: 92.47033201889425
100% 100/100 [30:24<00:00, 18.25s/it]
Model: gpt2-large_lora_epoch_2 BLEU Score: 93.404654446348